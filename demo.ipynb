{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification,AutoModelForCausalLM, AutoTokenizer,TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# 加载 Llama2-7B\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\"\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=1)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,  # LoRA rank\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"  # 适用于 GPT 类模型\n",
    ")\n",
    "model = get_peft_model(model, lora_config)  # 应用 LoRA\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haoyuhuang/opt/miniconda3/envs/ata/lib/python3.8/site-packages/peft/mapping.py:172: UserWarning: The PEFT config's `base_model_name_or_path` was renamed from 'google-bert/bert-base-chinese' to 'None'. Please ensure that the correct base model is loaded when loading this checkpoint.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 配置 LoRA 适配层\n",
    "# 应用 LoRA\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = model.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\").view(-1).float().to(device)  # ✅ 1D float\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits.view(-1)  # ✅ 1D logits\n",
    "\n",
    "        # 计算 BCEWithLogitsLoss\n",
    "        loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "        loss = loss_fn(logits, labels)\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 6/6 [00:00<00:00, 223.85 examples/s]\n",
      "                                             \n",
      " 20%|██        | 1/5 [00:07<00:23,  5.95s/it]Checkpoint destination directory ./results/checkpoint-1 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7300662994384766, 'eval_runtime': 1.5266, 'eval_samples_per_second': 3.93, 'eval_steps_per_second': 0.655, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \n",
      " 40%|████      | 2/5 [00:14<00:19,  6.55s/it]Checkpoint destination directory ./results/checkpoint-2 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7275132536888123, 'eval_runtime': 1.6185, 'eval_samples_per_second': 3.707, 'eval_steps_per_second': 0.618, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \n",
      " 60%|██████    | 3/5 [00:21<00:13,  6.70s/it]Checkpoint destination directory ./results/checkpoint-3 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7256802916526794, 'eval_runtime': 1.529, 'eval_samples_per_second': 3.924, 'eval_steps_per_second': 0.654, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \n",
      " 80%|████████  | 4/5 [00:28<00:06,  6.76s/it]Checkpoint destination directory ./results/checkpoint-4 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7244434356689453, 'eval_runtime': 1.5554, 'eval_samples_per_second': 3.858, 'eval_steps_per_second': 0.643, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \n",
      "100%|██████████| 5/5 [00:35<00:00,  6.80s/it]Checkpoint destination directory ./results/checkpoint-5 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "100%|██████████| 5/5 [00:35<00:00,  7.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7238247394561768, 'eval_runtime': 1.5935, 'eval_samples_per_second': 3.765, 'eval_steps_per_second': 0.628, 'epoch': 5.0}\n",
      "{'train_runtime': 35.1354, 'train_samples_per_second': 0.854, 'train_steps_per_second': 0.142, 'train_loss': 0.7388360500335693, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5, training_loss=0.7388360500335693, metrics={'train_runtime': 35.1354, 'train_samples_per_second': 0.854, 'train_steps_per_second': 0.142, 'train_loss': 0.7388360500335693, 'epoch': 5.0})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# 3️⃣ 处理数据\n",
    "def preprocess_function(examples):\n",
    "    inputs = tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "    inputs[\"labels\"] = [float(label) for label in examples[\"label\"]]  # 确保 label 是 float\n",
    "    return inputs\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files={\"train\": \"train.json\", \"test\": \"train.json\"})\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# 4️⃣ 训练参数\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    logging_dir=\"./logs\",\n",
    "    learning_rate=2e-5\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# 6️⃣ 训练\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def predict(text):\n",
    "    # 确保模型在正确的设备上\n",
    "    model.eval()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # 预处理输入文本\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=512).to(device)\n",
    "    \n",
    "    # 计算 logits（模型原始输出）\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits  # logits 形状: (1, 1)\n",
    "        prob = torch.sigmoid(logits)  # 计算概率（0~1）\n",
    "\n",
    "    return prob.item()  # 返回单个数值\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text1 逾期概率: 0.4461\n",
      "Text2 逾期概率: 0.4351\n"
     ]
    }
   ],
   "source": [
    "# **测试预测**\n",
    "text1 = \"用户在过去三个月内多次逾期还款，最近一次支付金额低于最低还款额\"\n",
    "text2 = \"用户信用评分良好，稳定工作，房贷已结清\"\n",
    "\n",
    "print(f\"Text1 逾期概率: {predict(text1):.4f}\")  \n",
    "print(f\"Text2 逾期概率: {predict(text2):.4f}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ata",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
